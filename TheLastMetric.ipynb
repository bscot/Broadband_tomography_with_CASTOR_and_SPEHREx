{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bscot/Broadband_tomography_with_CASTOR_and_SPEHREx/blob/main/TheLastMetric.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pzflow import Flow\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "from astropy.table import Table\n",
        "\n",
        "from pzflow import Flow, FlowEnsemble\n",
        "from pzflow.distributions import Uniform\n",
        "from pzflow.bijectors import Chain, StandardScaler, NeuralSplineCoupling\n",
        "\n",
        "\n",
        "# load data\n",
        "\n",
        "def getTrueY(test_cat, mag_col_names, y_col_name):\n",
        "\n",
        "    test_cat = Table(test_cat, masked=True, copy=True)\n",
        "    # remove nans\n",
        "    for col in mag_col_names:\n",
        "        test_cat[col].mask = np.isnan(test_cat[col].data) | test_cat[col].mask\n",
        "        test_cat = test_cat[~test_cat[col].mask] # then remove nans from test set\n",
        "            \n",
        "    true_y = test_cat[y_col_name]\n",
        "    return true_y.filled()\n",
        "\n",
        "unp = Table.read(\"unperturbed_mags.fits\")\n",
        "mock = Table.read(\"ext_phot.fits\")\n",
        "mask = unp[\"Euclid_VIS_MAG\"] <= 24.5\n",
        "\n",
        "CASTOR_baseline = mock[mask] #ignoring training data outside the Euclid flux limit \n",
        "\n",
        "names_phot = [\"ID\", \"photoz\",\n",
        "              \"LSST_g_MAG\", \"LSST_g_MAGERR\", \n",
        "              \"LSST_r_MAG\", \"LSST_r_MAGERR\",\n",
        "              \"LSST_i_MAG\", \"LSST_i_MAGERR\", \n",
        "              \"LSST_z_MAG\", \"LSST_z_MAGERR\",\n",
        "              \"castor_uv_MAG\", \"castor_uv_MAGERR\", \n",
        "              \"castor_u_MAG\", \"castor_u_MAGERR\", \n",
        "              \"castor_g_MAG\", \"castor_g_MAGERR\"]\n",
        "\n",
        "available_os = [\"baseline\"]\n",
        "names = [\n",
        "    \"baseline\",\n",
        "]\n",
        "\n",
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\"] #, \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))\n",
        "\n",
        "# put data in expected format for TLM \n",
        "\n",
        "LSST_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAG\")\n",
        "LSST_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAG\")\n",
        "LSST_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAG\")\n",
        "LSST_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAG\")\n",
        "\n",
        "LSST_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAGERR\")\n",
        "LSST_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAGERR\")\n",
        "LSST_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAGERR\")\n",
        "LSST_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAGERR\")\n",
        "\n",
        "CASTOR_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_g_MAG\")\n",
        "CASTOR_u_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_u_MAG\")\n",
        "CASTOR_uv_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_uv_MAG\")\n",
        "\n",
        "CASTOR_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_g_MAGERR\")\n",
        "CASTOR_u_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_u_MAGERR\")\n",
        "CASTOR_uv_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"castor_uv_MAGERR\")\n",
        "\n",
        "ID = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"ID\")\n",
        "z_true = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"photoz\")\n",
        "\n",
        "#df = pd.DataFrame({'CASTOR_ID': data_array_sorted_CASTOR_wID[:,0] , 'error': data_array_sorted_CASTOR_wID[:,1], \n",
        "#                   'g-r': data_array_sorted_CASTOR_wID[:,2] - data_array_sorted_CASTOR_wID[:,3], \n",
        "#                   'r-i': data_array_sorted_CASTOR_wID[:,3] - data_array_sorted_CASTOR_wID[:,4], \n",
        "#                   'i-z': data_array_sorted_CASTOR_wID[:,4] - data_array_sorted_CASTOR_wID[:,5], \n",
        "#                   'true_z': data_array_sorted_CASTOR_wID[:,6]})\n",
        "\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    \n",
        "#     cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "#                              'LSST_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR, \n",
        "#                              'LSST_r_mag': LSST_r_mag, 'LSST_r_mag_ERR': LSST_r_mag_ERR, \n",
        "#                              'LSST_i_mag': LSST_i_mag, 'LSST_i_mag_ERR': LSST_i_mag_ERR, \n",
        "#                              'LSST_z_mag': LSST_z_mag, 'LSST_z_mag_ERR': LSST_z_mag_ERR, \n",
        "#                              'CASTOR_uv_mag': LSST_uv_mag, 'LSST_uv_mag_ERR': LSST_uv_mag_ERR, \n",
        "#                              'CASTOR_u_mag': LSST_u_mag, 'LSST_u_mag_ERR': LSST_u_mag_ERR, \n",
        "#               'CASTOR_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR})\n",
        "    \n",
        "    # this will need to change to accomodate multiple catalogs, e.g. LSST only or LSST + CASTOR\n",
        "    \n",
        "    cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "                             'r': LSST_r_mag,\n",
        "                             'g-r': LSST_g_mag - LSST_r_mag, \n",
        "                             'r-i': LSST_r_mag - LSST_i_mag, \n",
        "                             'i-z': LSST_i_mag - LSST_z_mag, \n",
        "                             'uv-u': CASTOR_uv_mag - CASTOR_u_mag, \n",
        "                             'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "                      })\n",
        "    \n",
        "    catalogs[os] = cat.dropna()\n",
        "\n",
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform(1, 5) # did the syntax here change?\n",
        "\n",
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"uv - u\"] # different colors than LSST  \n",
        "\n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "\n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "\n",
        "    ensembles[os] = flowEns\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)\n"
      ],
      "metadata": {
        "id": "l7J0rqGeC_d4",
        "outputId": "e92a61fc-52b4-455f-e866-1ccbe44fbaf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        }
      },
      "id": "l7J0rqGeC_d4",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-ac1de9c84d71>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    ​\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a9be070",
      "metadata": {
        "id": "9a9be070"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f49e3a2b",
      "metadata": {
        "id": "f49e3a2b"
      },
      "source": [
        "dependencies: pzflow, pandas, jax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "83ab7905",
      "metadata": {
        "id": "83ab7905",
        "outputId": "fe72e6a8-88d7-4aec-9242-7cedee28052f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e6a6cda5d006>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpzflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFlow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pzflow'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from pzflow import Flow\n",
        "import jax.numpy as jnp\n",
        "import pandas as pd \n",
        "\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a16a1f",
      "metadata": {
        "id": "43a16a1f"
      },
      "outputs": [],
      "source": [
        "from astropy.table import Table"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42ed4f28",
      "metadata": {
        "id": "42ed4f28"
      },
      "source": [
        "## CASTOR COSMOS synthetic catalogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff5d07a",
      "metadata": {
        "id": "8ff5d07a"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "unp = Table.read(\"unperturbed_mags.fits\")\n",
        "mock = Table.read(\"ext_phot.fits\")\n",
        "mask = unp[\"Euclid_VIS_MAG\"] <= 24.5\n",
        "\n",
        "CASTOR_baseline = mock[mask] #ignoring training data outside the Euclid flux limit \n",
        "\n",
        "names_phot = [\"ID\", \"photoz\",\n",
        "              \"LSST_g_MAG\", \"LSST_g_MAGERR\", \n",
        "              \"LSST_r_MAG\", \"LSST_r_MAGERR\",\n",
        "              \"LSST_i_MAG\", \"LSST_i_MAGERR\", \n",
        "              \"LSST_z_MAG\", \"LSST_z_MAGERR\"\n",
        "              \"castor_uv_MAG\", \"castor_uv_MAGERR\", \n",
        "              \"castor_u_MAG\", \"castor_u_MAGERR\", \n",
        "              \"castor_g_MAG\", \"castor_g_MAGERR\"]\n",
        "\n",
        "available_os = [\"baseline\"]\n",
        "names = [\n",
        "    \"baseline\",\n",
        "]\n",
        "\n",
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\"] #, \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))\n",
        "\n",
        "# put data in expected format for TLM \n",
        "\n",
        "LSST_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAG\")\n",
        "LSST_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAG\")\n",
        "LSST_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAG\")\n",
        "LSST_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAG\")\n",
        "\n",
        "LSST_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_g_MAGERR\")\n",
        "LSST_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_r_MAGERR\")\n",
        "LSST_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_i_MAGERR\")\n",
        "LSST_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"LSST_z_MAGERR\")\n",
        "\n",
        "CASTOR_g_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_g_MAG\")\n",
        "CASTOR_r_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_r_MAG\")\n",
        "CASTOR_i_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_i_MAG\")\n",
        "CASTOR_z_mag = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_z_MAG\")\n",
        "\n",
        "CASTOR_g_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_g_MAGERR\")\n",
        "CASTOR_r_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_r_MAGERR\")\n",
        "CASTOR_i_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_i_MAGERR\")\n",
        "CASTOR_z_mag_ERR = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"CASTOR_z_MAGERR\")\n",
        "\n",
        "ID = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"ID\")\n",
        "z_true = getTrueY(test_cat=CASTOR_baseline, mag_col_names=names_phot, y_col_name=\"photoz\")\n",
        "\n",
        "df = pd.DataFrame({'CASTOR_ID': data_array_sorted_CASTOR_wID[:,0] , 'error': data_array_sorted_CASTOR_wID[:,1], \n",
        "                   'g-r': data_array_sorted_CASTOR_wID[:,2] - data_array_sorted_CASTOR_wID[:,3], \n",
        "                   'r-i': data_array_sorted_CASTOR_wID[:,3] - data_array_sorted_CASTOR_wID[:,4], \n",
        "                   'i-z': data_array_sorted_CASTOR_wID[:,4] - data_array_sorted_CASTOR_wID[:,5], \n",
        "                   'true_z': data_array_sorted_CASTOR_wID[:,6]})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db5e24e",
      "metadata": {
        "id": "1db5e24e"
      },
      "outputs": [],
      "source": [
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    \n",
        "#     cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "#                              'LSST_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR, \n",
        "#                              'LSST_r_mag': LSST_r_mag, 'LSST_r_mag_ERR': LSST_r_mag_ERR, \n",
        "#                              'LSST_i_mag': LSST_i_mag, 'LSST_i_mag_ERR': LSST_i_mag_ERR, \n",
        "#                              'LSST_z_mag': LSST_z_mag, 'LSST_z_mag_ERR': LSST_z_mag_ERR, \n",
        "#                              'CASTOR_uv_mag': LSST_uv_mag, 'LSST_uv_mag_ERR': LSST_uv_mag_ERR, \n",
        "#                              'CASTOR_u_mag': LSST_u_mag, 'LSST_u_mag_ERR': LSST_u_mag_ERR, \n",
        "#               'CASTOR_g_mag': LSST_g_mag, 'LSST_g_mag_ERR': LSST_g_mag_ERR})\n",
        "    \n",
        "    # this will need to change to accomodate multiple catalogs, e.g. LSST only or LSST + CASTOR\n",
        "    \n",
        "    cat = pd.DataFrame({'CASTOR_ID': ID, 'z_true': z_true, \n",
        "                             'r': LSST_r_mag,\n",
        "                             'g-r': LSST_g_mag - LSST_r_mag, \n",
        "                             'r-i': LSST_r_mag - LSST_i_mag, \n",
        "                             'i-z': LSST_i_mag - LSST_z_mag, \n",
        "                             'uv-u': CASTOR_uv_mag - CASTOR_u_mag; \n",
        "                             'u-g': CASTOR_u_mag - CASTOR_g_mag\n",
        "                      })\n",
        "    \n",
        "    catalogs[os] = cat.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2933ee82",
      "metadata": {
        "id": "2933ee82"
      },
      "source": [
        "# Instantiate and Train \n",
        "\n",
        "### Does this run 'as is'?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c66cfd3",
      "metadata": {
        "id": "4c66cfd3"
      },
      "outputs": [],
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform((-5, 5))\n",
        "\n",
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"uv - u\"] # different colors than LSST  \n",
        "\n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "\n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "\n",
        "    ensembles[os] = flowEns\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "%%time\n",
        "\n",
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27fcca35",
      "metadata": {
        "id": "27fcca35"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/training_flows_July23.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c392b800",
      "metadata": {
        "id": "c392b800"
      },
      "source": [
        "pull the data just to get formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18074f8",
      "metadata": {
        "id": "f18074f8"
      },
      "outputs": [],
      "source": [
        "# !wget https://storage.googleapis.com/ahw2019/for_malz_and_lanusse.tar.gz\n",
        "# !tar -xzf for_malz_and_lanusse.tar.gz\n",
        "# !mv for_malz_and_lanusse dataset\n",
        "\n",
        "\n",
        "\n",
        "more dataset/readme.txt\n",
        "\n",
        "\n",
        "\n",
        "# list of available catalogs\n",
        "available_os = [\"run_1_4_y10\", \"run_4_38_y10\", \"run_10_92_y10\", \"run_4_34_y10\", \"run_7_61_y10\", \"run_9_86_y10\"]\n",
        "names = [\n",
        "    \"baseline_v1_5_10yrs\",\n",
        "    \"footprint_stuck_rollingv1_5_10yrs\",\n",
        "    \"ddf_heavy_nexp2_v1_6_10yrs\",\n",
        "    \"footprint_newAv1_5_10yrs\",\n",
        "    \"third_obs_pt60v1_5_10yrs\",\n",
        "    \"barebones_v1_6_10yrs\",\n",
        "]\n",
        "os_names = dict(zip(available_os, names))\n",
        "colors = [\"k\", \"plum\", \"cornflowerblue\", \"#2ca02c\", \"gold\", \"tomato\"]\n",
        "os_colors = dict(zip(available_os, colors))\n",
        "\n",
        "\n",
        "# column names of the catalogs\n",
        "names_z=('ID', 'z_true', 'z_phot', 'dz_phot', 'NN', 'N_train')\n",
        "names_phot=(\n",
        "    'ID', 'z_true', \n",
        "    'u', 'u_err',\n",
        "    'g', 'g_err',\n",
        "    'r', 'r_err',\n",
        "    'i', 'i_err',\n",
        "    'z', 'z_err',\n",
        "    'y', 'y_err',\n",
        "    'u-g', 'u-g_err',\n",
        "    'g-r', 'g-r_err',\n",
        "    'r-i', 'r-i_err',\n",
        "    'i-z', 'i-z_err',\n",
        "    'z-y', 'z-y_err',\n",
        ")\n",
        "\n",
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    z_cat = pd.read_csv(f\"dataset/{os}/zphot.cat\", names=names_z, delim_whitespace=True, skiprows=1)\n",
        "    phot_cat = pd.read_csv(f\"dataset/{os}/test.cat\", names=names_phot, delim_whitespace=True)\n",
        "    cat = z_cat.merge(phot_cat)\n",
        "    catalogs[os] = cat.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c19b11f7",
      "metadata": {
        "id": "c19b11f7"
      },
      "source": [
        "instantiate the model of the photometric space, train it on the catalogs for LSST with and without CASTOR, and save it (two versions rather than 6 OSs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8520d4e2",
      "metadata": {
        "id": "8520d4e2"
      },
      "outputs": [],
      "source": [
        "# first I create a bijector chain\n",
        "# the first bijection is a standard scaler - but I'm not actually using it for standard scaling\n",
        "#     I set the mean and std so that it maps the redshift range (0, 3.2) onto (-5, 5), which is \n",
        "#     the domain of the NeuralSplineCoupling\n",
        "# the second bijection is a NeuralSplineCoupling. I told it to expect 6 conditions,\n",
        "#     which will be the r mag and the galaxy colors\n",
        "bijector = Chain(\n",
        "    StandardScaler(np.atleast_1d(1.6), np.atleast_1d(0.32)),\n",
        "    NeuralSplineCoupling(n_conditions=6)\n",
        ")\n",
        "\n",
        "# I set the latent distribution to a Uniform over (-5, 5)\n",
        "# this range was chosen to match the NeuralSplineCoupling domain\n",
        "# I chose a Uniform since all of the redshifts are drawn from a compact domain\n",
        "latent = Uniform((-5, 5))\n",
        "\n",
        "# create a dictionary that will hold all the ensembles\n",
        "ensembles = dict()\n",
        "\n",
        "# create the baseline flows\n",
        "for os in available_os:\n",
        "\n",
        "    # the data column is the one that is sampled and transformed by the flow\n",
        "    data_columns = [\"z_true\"]\n",
        "    # the conditional columns are the columns that the flow is conditioned on\n",
        "    conditional_columns = [\"r\", \"u-g\", \"g-r\", \"r-i\", \"i-z\", \"z-y\"]\n",
        "\n",
        "    # save some info with the flow\n",
        "    info = f\"Models z_true conditioned on galaxy colors and r mag from os {os}. K=16\"\n",
        "\n",
        "    # instantiate and save the flow\n",
        "    flowEns = FlowEnsemble(data_columns = data_columns, \n",
        "                           conditional_columns = conditional_columns,\n",
        "                           bijector = bijector,\n",
        "                           latent = latent,\n",
        "                           info = info,\n",
        "                           N = 10)\n",
        "\n",
        "    ensembles[os] = flowEns\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "%%time\n",
        "\n",
        "for os, ens in ensembles.items():\n",
        "    \n",
        "    # get the data and make a train and test set\n",
        "    cat = catalogs[os]\n",
        "    cat_train = cat.sample(frac = 0.8)\n",
        "    cat_test = cat.drop(cat_train.index)\n",
        "    \n",
        "    # train the flow on the given learning rate schedule\n",
        "    loss1 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-3),\n",
        "                       epochs = 100, seed = 123)\n",
        "    loss2 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 2e-4),\n",
        "                       epochs = 100, seed = 312)\n",
        "    loss3 = ens.train(cat_train, sample_errs=True,\n",
        "                       optimizer = adam(step_size = 1e-4),\n",
        "                       epochs = 50, seed = 231)\n",
        "    \n",
        "    losses = {fname : # for each flow trained in the ensemble...\n",
        "                  [float(loss) # save the list of training losses\n",
        "                   for lossDict in [loss1, loss2, loss3]\n",
        "                   for loss in lossDict[fname]]\n",
        "              for fname in loss1}\n",
        "    \n",
        "    # print the train and test loss\n",
        "    train_loss = -np.mean(ens.log_prob(cat_train))\n",
        "    test_loss = -np.mean(ens.log_prob(cat_test))\n",
        "    print(os, train_loss, test_loss)\n",
        "    \n",
        "    # save the ensemble\n",
        "    ens.save(f\"trained_flows/pzflow_ensemble_for_{os}.pkl\")\n",
        "    # and the losses\n",
        "    with open(f\"trained_flows/losses_for_{os}.pkl\", \"wb\") as file:\n",
        "        pickle.dump({\"losses\": losses, \n",
        "                     \"train loss\": train_loss, \n",
        "                     \"test loss\": test_loss},\n",
        "                    file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab4b8090",
      "metadata": {
        "id": "ab4b8090"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/MAFVariationalMutualInformationPzFlow.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d935862c",
      "metadata": {
        "id": "d935862c"
      },
      "source": [
        "open the models of redshift-photometry space and the catalogs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35f28566",
      "metadata": {
        "id": "35f28566"
      },
      "outputs": [],
      "source": [
        "flows = {}\n",
        "for os in available_os:\n",
        "  flows[os] = Flow(file=f\"trained_flows/flow_for_run_{os}.pkl\")\n",
        "# TODO: need to experiment with different fit parameters because this might be too smooth, also does it account for photometric errors?\n",
        "# TODO: check that draws from flow look like original data\n",
        "\n",
        "\n",
        "# load the catalogs\n",
        "catalogs = dict()\n",
        "for os in available_os:\n",
        "    z_cat = pd.read_csv(f\"dataset/run_{os}/zphot.cat\", names=names_z, delim_whitespace=True, skiprows=1)\n",
        "    phot_cat = pd.read_csv(f\"dataset/run_{os}/test.cat\", names=names_phot, delim_whitespace=True)\n",
        "    cat = z_cat.merge(phot_cat)\n",
        "    catalogs[os] = cat.dropna()\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15e77d45",
      "metadata": {
        "id": "15e77d45"
      },
      "source": [
        "evaluate the posteriors for each galaxy (totally forgot we had to do this)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "197e07f1",
      "metadata": {
        "id": "197e07f1"
      },
      "outputs": [],
      "source": [
        "# # this just makes the posteriors for plotting, not sure why it uses so much memory. . .\n",
        "# tx = np.linspace(0,3.5,100)\n",
        "# all_logp = {}\n",
        "# for which_os in available_os:\n",
        "#   flow = flows[which_os]\n",
        "#   cat = catalogs[which_os]\n",
        "#   logp = flow.posterior(flow.info[\"condition_scaler\"](cat), column=\"z_true\", grid=tx)\n",
        "#   all_logp[which_os] = logp\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ff727de",
      "metadata": {
        "id": "5ff727de"
      },
      "outputs": [],
      "source": [
        "all_milb = {}\n",
        "for which_os in available_os:\n",
        "  phot_cat = catalogs[which_os]\n",
        "\n",
        "  mutual_information_lower_bound = flows[which_os].log_prob(flows[which_os].info[\"condition_scaler\"](phot_cat))\n",
        "  all_milb[which_os] = mutual_information_lower_bound\n",
        "  print((os_names[which_os], np.sum(mutual_information_lower_bound)))\n",
        "# TODO: make this an actual expected value rather than just sum\n",
        "# also, shouldn't it be sum of exponential of metric value, since it should never penalize a negative value?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bde34f5",
      "metadata": {
        "id": "7bde34f5"
      },
      "source": [
        "KEY PLOT: distribution of metric values for same galaxies with and without CASTOR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d65b56f",
      "metadata": {
        "id": "6d65b56f"
      },
      "outputs": [],
      "source": [
        "# surprisingly not so different from one another\n",
        "for which_os in available_os:\n",
        "  mutual_information_lower_bound = all_milb[which_os].flatten()\n",
        "  print((np.mean(mutual_information_lower_bound), np.std(mutual_information_lower_bound)))\n",
        "  hist(mutual_information_lower_bound, bins=np.linspace(-16, 5, 100), alpha=0.5, histtype='step', \n",
        "       color=os_colors[which_os], label=os_names[which_os], density=False)\n",
        "  xlabel(r'$\\mathbb{E}_{z, x_{phot}} \\left[ q_\\theta(z | x_{phot}) \\right]$')\n",
        "# xlim(-10., 5.)\n",
        "legend(loc='upper left')\n",
        "# semilogy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca64b1a0",
      "metadata": {
        "id": "ca64b1a0"
      },
      "source": [
        "KEY PLOT: look at per-galaxy metric values as a function of redshift, then photometry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b500416",
      "metadata": {
        "id": "6b500416"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(len(available_os), 1, figsize=(5, 5*len(available_os)))\n",
        "for i, which_os in enumerate(available_os):\n",
        "  axs[i].hist2d(z_cats[which_os]['z_true'], all_milb[which_os].flatten(), bins=[np.linspace(0., 3., 50), np.linspace(-5., 5., 100)])\n",
        "  axs[i].set_xlabel('redshift')\n",
        "  axs[i].set_ylabel(r'$\\mathbb{E}_{z, x_{phot}} \\left[ q_\\theta(z | x_{phot}) \\right]$')\n",
        "  axs[i].set_title(os_names[which_os])\n",
        "# they're different, but not visibly so\n",
        "# TODO: plot violins of metric as a function of binned redshift so they're all on one set of axes? or quantiles because outlers? or box/whisker https://matplotlib.org/stable/gallery/pyplots/boxplot_demo_pyplot.html?\n",
        "# TODO: normalize within redshift bins to get these on one set of axes?\n",
        "# TODO sort of want sum of metric values in redshift bins, no?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "680f3fbc",
      "metadata": {
        "id": "680f3fbc"
      },
      "source": [
        "take expected value of the approximated posteriors, which is the mutual information lower bound"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0c46d85",
      "metadata": {
        "id": "c0c46d85"
      },
      "outputs": [],
      "source": [
        "# something isn't right about the autocalculation of moments so doing it by hand\n",
        "def calc_moment(vals, k):\n",
        "  n = len(vals)\n",
        "  outval = np.sum(vals**k) / float(n)\n",
        "  return float(outval)\n",
        "\n",
        "which_moments = range(0, 5)\n",
        "moment_res = {}\n",
        "for which_os in available_os:\n",
        "  # print((np.mean(all_milb[which_os]), np.std(all_milb[which_os])))\n",
        "  moment_res[which_os] = []\n",
        "  for i in which_moments:\n",
        "    moment_res[which_os].append(calc_moment(all_milb[which_os], k=i))#sps.mstats.moment(all_milb[which_os], moment=which_moments[i], axis=0))\n",
        "# print(moment_res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a67574d",
      "metadata": {
        "id": "1a67574d"
      },
      "source": [
        "# from github/aimalz/TheLastMetric/blob/master/FigureTLMvsRedshift.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea32a245",
      "metadata": {
        "id": "ea32a245"
      },
      "source": [
        "KEY PLOT: distribution of TLM values with and without CASTOR including model uncertainty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9f064b1",
      "metadata": {
        "id": "c9f064b1"
      },
      "outputs": [],
      "source": [
        "%pylab inline\n",
        "from utils import load_data, compute_last_metric\n",
        "import corner\n",
        "from pzflow import Flow\n",
        "\n",
        "\n",
        "# Loading data\n",
        "z_cats, phot_cats, available_os, os_names, os_colors = load_data()\n",
        "\n",
        "\n",
        "# Loading pre-trained flows\n",
        "flows = {}\n",
        "for os in available_os:\n",
        "  flows[os] = [Flow(file=f\"trained_flows/flow_for_run_{os}_%d.pkl\"%(i+1) ) for i in range(10)]\n",
        "\n",
        "\n",
        "# Computing metric for each observing strategy\n",
        "all_tlm = {}\n",
        "for which_os in available_os:\n",
        "  all_tlm[which_os] = np.stack([(compute_last_metric(f,\n",
        "                                          phot_cats[which_os],\n",
        "                                          z_cats[which_os], entropy_nbins=60)) for f in flows[which_os] ], axis=0)\n",
        "  print((os_names[which_os], np.mean(all_tlm[which_os]), np.std(np.mean(all_tlm[which_os], axis=1))))\n",
        "    \n",
        "    \n",
        "    \n",
        "\n",
        "figure(figsize=(7,5))\n",
        "for which_os in available_os:\n",
        "  hist(np.mean(all_tlm[which_os],axis=1), 32, range=[2.95, 3.35], alpha=0.6,\n",
        "       color=os_colors[which_os], label=os_names[which_os])\n",
        "  axvline(np.mean(all_tlm[which_os]), color=os_colors[which_os], linewidth=2)\n",
        "legend(ncol=2)\n",
        "xlabel(chr(0x05ea))\n",
        "savefig('metrics.pdf', bbox_inches = 'tight', pad_inches = 0 )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}